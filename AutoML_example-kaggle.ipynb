{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onepanel AutoML 0.1a - Kaggle Dataset Example\n",
    "\n",
    "Here we use AutoML to solve a classification task on a classic [Titanic](https://www.kaggle.com/c/titanic) dataset from Kaggle. First, let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived Pclass  \\\n",
       "0            1         0      3   \n",
       "1            2         1      1   \n",
       "2            3         1      3   \n",
       "3            4         1      1   \n",
       "4            5         0      3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/train.csv', parse_dates=[2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdubovikov/miniconda3/envs/automl/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "# AutoML uses Python's logging module\n",
    "import logging\n",
    "\n",
    "# Various sklearn models and metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# AutoML Clasees\n",
    "from automl.pipeline import LocalExecutor, Pipeline, PipelineStep, PipelineData\n",
    "from automl.data.dataset import Dataset\n",
    "from automl.model import ModelSpace, CV, Validate, ChooseBest\n",
    "from automl.hyperparam.templates import (random_forest_hp_space, \n",
    "                                         knn_hp_space, svc_kernel_hp_space, \n",
    "                                         grad_boosting_hp_space, \n",
    "                                         xgboost_hp_space)\n",
    "from automl.feature.generators import FormulaFeatureGenerator, PolynomialGenerator\n",
    "from automl.feature.selector import FeatureSelector\n",
    "from automl.hyperparam.optimization import Hyperopt\n",
    "from automl.combinators import RandomChoice\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how automated our process is, data still may need some preprocessing. Also, doing good old feature engenering can help by a lot. We skip exploratory data analysis and feature engeneering stages for brevity. If you are interested, we suggest looking up some examples at contest's [kernels](https://www.kaggle.com/c/titanic/kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data and create AutoML Dataset\"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    result = df.copy()\n",
    "    \n",
    "    # drop columns we won't be using\n",
    "    result.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "    \n",
    "    # transform Sex column into numeric categories\n",
    "    result['Sex'] = encoder.fit_transform(result['Sex'])\n",
    "    \n",
    "    # do the same with Embarked column\n",
    "    result['Embarked'] = encoder.fit_transform(result['Embarked'].astype(str))\n",
    "    \n",
    "    # Replace missing Ages with median value and \"pack\"\n",
    "    # Age into 10 equal-sized bins. For example, all \n",
    "    # ages from 0-10 will be packed into bin 0.\n",
    "    result['Age'].fillna(result['Age'].median(), inplace=True)\n",
    "    result['Age'] = pd.cut(result['Age'], 10, labels=range(0,10)).astype(int)\n",
    "    \n",
    "    # Pack Fare into 10 bins\n",
    "    result['Fare'] = pd.cut(result['Fare'], 10, labels=range(0,10)).astype(int)\n",
    "    \n",
    "    # transform Pclass type to int\n",
    "    result['Pclass'] = result['Pclass'].astype(int)\n",
    "    \n",
    "    # add some useful predictive features that may came \n",
    "    # up to mind during data analysis\n",
    "    result['FamilySize'] = result['SibSp'] + result['Parch'] + 1\n",
    "    result['IsAlone'] = 0\n",
    "    result.loc[result['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    return Dataset(result.drop(['Survived'], axis=1),\n",
    "                   result['Survived'])\n",
    "\n",
    "dataset = preprocess_data(data)\n",
    "print(f\"Features: {dataset.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit simple XGBoost model with default parameters and see how it scores on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81144781144781142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf = XGBClassifier()\n",
    "np.mean(cross_val_score(rf, dataset.data, dataset.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 81% accuracy with the defaults. Let's go on to AutoML Pipelines and see if we can improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'SklearnFeatureGenerator'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x10faeee10>, 'max_features': <hyperopt.pyll.base.Apply object at 0x10faf2208>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x10faf2518>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x10faf28d0>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x10b6cf898>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x10fb36da0>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004532 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003844 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005563 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004611 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.009106 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003564 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004694 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004207 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004699 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003657 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005177 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003759 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004529 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003736 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.008386 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003577 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007894 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004632 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004655 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003530 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.177329\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x10faf2b00>, 'weights': <hyperopt.pyll.base.Apply object at 0x10faf2c18>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001352 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001831 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.261504\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002380 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002091 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002128 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001933 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001750 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001671 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001796 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001902 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002023 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002047 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001760 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001939 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002023 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001689 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002291 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001720 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003277 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.228956\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002537 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.228956\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x10faf2e10>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x10faf2f98>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x10faf41d0>, 'gamma': <hyperopt.pyll.base.Apply object at 0x10faf4358>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x10faf4518>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10faf4668>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x10faf47b8>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x10faf4908>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x10faf4a90>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x10faf4c18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x10faf4cc0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004621 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005268 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.383838\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005351 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.208754\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005055 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004515 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005019 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004736 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005304 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004378 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004522 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005222 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005232 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005586 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005373 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004388 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004785 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005130 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004458 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005925 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.184063\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004541 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.184063\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [02:41<01:47, 53.73s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features=0.5499452045517615,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1112, n_jobs=1, oob_score=False, random_state=3,\n",
      "            verbose=False, warm_start=False) - 0.8226711560044894\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5098180092840814,\n",
      "       colsample_bytree=0.6444833561839967, gamma=0.0003081711916719362,\n",
      "       learning_rate=0.001618181538735828, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=3, missing=None, n_estimators=1600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.5185412218041878, reg_lambda=1.0766843075603876,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.6962178872658329) - 0.8159371492704826\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=14, p=2,\n",
      "           weights='distance') - 0.7710437710437711\n",
      "LocalExecutor - INFO - Running step 'FeatureSelector'\n",
      "FeatureSelector - INFO - Removing 20 features for model RandomForestClassifier\n",
      "100%|██████████| 5/5 [02:41<00:00, 32.27s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #2\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'SklearnFeatureGenerator'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x10faeee10>, 'max_features': <hyperopt.pyll.base.Apply object at 0x10faf2208>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x10faf2518>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x10faf28d0>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x10b6cf898>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x10fb36da0>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005598 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005455 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003532 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005614 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005191 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004047 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004131 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003693 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004896 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004305 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004138 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004964 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003864 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003724 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004548 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.185185\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007464 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.175084\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.009362 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.175084\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005426 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.175084\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005156 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.175084\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005312 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.172840\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x10faf2b00>, 'weights': <hyperopt.pyll.base.Apply object at 0x10faf2c18>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001448 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002330 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003565 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002696 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003490 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.226712\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002940 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.226712\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002790 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.226712\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002918 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.226712\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003412 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.226712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - tpe_transform took 0.002627 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.226712\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003065 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002142 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002302 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002197 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003900 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002152 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002580 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002844 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002321 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.222222\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002522 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.222222\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x10faf2e10>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x10faf2f98>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x10faf41d0>, 'gamma': <hyperopt.pyll.base.Apply object at 0x10faf4358>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x10faf4518>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10faf4668>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x10faf47b8>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x10faf4908>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x10faf4a90>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x10faf4c18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x10faf4cc0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004840 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005901 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004710 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006904 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006142 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004695 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004713 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004652 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005604 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005983 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005692 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004529 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004592 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004452 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004776 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.197531\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004656 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.191919\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004936 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.191919\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005445 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.191919\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006687 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.191919\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005244 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.191919\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [09:32<06:21, 190.99s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=55, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.8271604938271605\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6594960983446453,\n",
      "       colsample_bytree=0.639295323462608, gamma=0.07658810027926546,\n",
      "       learning_rate=0.004614029407255256, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=4, missing=None, n_estimators=1600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.318988030454243, reg_lambda=3.232750109230554,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.6785062055286775) - 0.8080808080808081\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
      "           weights='distance') - 0.7777777777777777\n",
      "LocalExecutor - INFO - Running step 'FeatureSelector'\n",
      "FeatureSelector - INFO - Removing 20 features for model RandomForestClassifier\n",
      "100%|██████████| 5/5 [09:32<00:00, 114.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=55, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) 0.8271604938271605\n",
      "(891, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, we define our ModelSpace. ModelSpace is initialized by a list of tuples.\n",
    "# First element of each tuple should be an sklearn-like estimator with fit method\n",
    "# The second one is model parameter dictionary. Here we do not define parameters \n",
    "# explicitly, but use hyperparameter templates from AutoML. Those templates can be\n",
    "# used later by Hyperopt step to find best model parameters automatically\n",
    "model_list = [\n",
    "      (RandomForestClassifier, random_forest_hp_space()),\n",
    "      (KNeighborsClassifier, knn_hp_space(lambda key: key)),\n",
    "      (XGBClassifier, xgboost_hp_space())\n",
    "  ]\n",
    "\n",
    "\n",
    "# Create executor, initialize it with our classification dataset \n",
    "# and set total number of epochs to 2 (the pipeline will be run two times in a row).\n",
    "# We can load any pipeline into executor using << operator like below:\n",
    "context, pipeline_data = LocalExecutor(dataset, epochs=2) << \\\n",
    "    (Pipeline() # Here we define the pipeline. Steps can be added to pipeline using >> operator\n",
    "     # First we define our ModelSpace. We wrap it with PipelineStep class \n",
    "     # and set initializer=True so that ModelSpace step will be run only at the first epoch\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     # But we are not obliged to wrap all steps with PipelineStep.\n",
    "     # This will be done automatically if we do not need to set any special parameters \n",
    "     # We use PolynomialGenerator to create polynomial combinations of the features from the dataset\n",
    "     >> PolynomialGenerator()\n",
    "     # Next we use Hyperopt to find the best combination of hyperparameters for each model\n",
    "     # We use test set validation with accuracy metric as a score function.\n",
    "     # CV could be used instead of Validate to perform cross-validation\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=20)\n",
    "     # Then we choose the best performing model we found\n",
    "     >> ChooseBest(1)\n",
    "     # And select 10 best features\n",
    "     >> FeatureSelector(20))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have reached better accuracy compared to default XGBoost plain dataset. However, first pipeline we launched had a good job of generating various features for our dataset, but it was not really created for searching the best model. Now let's create a pipeline which will search for the best model on a fixed dataset.\n",
    "\n",
    "Please note that increasing `max_evals` parameter for `Hyperopt` can lead to finding better model parameters, but we use modest values here for demonstation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x10faeee10>, 'max_features': <hyperopt.pyll.base.Apply object at 0x10faf2208>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x10faf2518>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x10faf28d0>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x10b6cf898>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x10fb36da0>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004155 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004064 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.210999\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003682 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003563 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003486 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003614 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003718 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003850 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003770 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003961 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004460 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003585 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005105 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004803 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005217 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005332 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003641 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004435 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004062 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004599 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004151 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.179574\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003550 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004911 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004954 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.172840\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004911 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.172840\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004487 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.172840\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004998 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005387 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003775 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004499 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005644 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004161 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003589 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003791 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003581 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004223 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004078 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003617 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.171717\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004453 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004046 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003667 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004755 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004435 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005392 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003972 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003829 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004739 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004586 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004347 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.170595\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005056 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.170595\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x10faf2b00>, 'weights': <hyperopt.pyll.base.Apply object at 0x10faf2c18>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001221 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002322 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003314 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002735 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002201 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003178 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002272 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002444 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.193042\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003462 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002921 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002146 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002798 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002656 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002248 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002222 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003496 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002232 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002338 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002851 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002683 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002826 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002173 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002261 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002453 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002740 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002228 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002287 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002933 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003012 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003214 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002203 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002436 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002082 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002205 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002291 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003020 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002887 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002502 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002288 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002152 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002261 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002113 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002282 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002659 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002237 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002279 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002270 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002340 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002290 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.177329\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003276 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.177329\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x10faf2e10>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x10faf2f98>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x10faf41d0>, 'gamma': <hyperopt.pyll.base.Apply object at 0x10faf4358>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x10faf4518>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10faf4668>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x10faf47b8>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x10faf4908>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x10faf4a90>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x10faf4c18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x10faf4cc0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004850 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004509 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.383838\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004865 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.225589\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004530 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004488 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004449 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004442 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004561 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005849 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004520 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004636 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005132 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.190797\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006152 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.190797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - tpe_transform took 0.005458 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.190797\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004400 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.190797\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004390 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.190797\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004481 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.189675\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004367 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.189675\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004798 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.189675\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004588 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005005 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004484 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004828 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005013 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005024 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004704 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005349 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006998 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004521 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004879 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005402 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004686 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004437 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004521 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004677 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004853 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004599 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004576 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006276 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004637 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006259 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004427 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004561 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004499 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004933 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004482 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005709 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004456 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004562 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005180 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.181818\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 67%|██████▋   | 2/3 [04:27<02:13, 133.78s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.8286699106837324,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=21,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1309, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.8294051627384961\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=23, p=2,\n",
      "           weights='uniform') - 0.8226711560044894\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6522725726190979,\n",
      "       colsample_bytree=0.9175023244484342, gamma=0.7655683232150291,\n",
      "       learning_rate=9.6330015915942e-07, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=3, missing=None, n_estimators=4800, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.00041301659758667615, reg_lambda=3.358766604190483,\n",
      "       scale_pos_weight=1, seed=3, silent=True,\n",
      "       subsample=0.6714766555950911) - 0.8181818181818182\n",
      "100%|██████████| 3/3 [04:27<00:00, 89.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.8286699106837324,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=21,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1309, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) 0.8294051627384961\n",
      "(891, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context, pipeline_data = LocalExecutor(pipeline_data.dataset, epochs=1) << \\\n",
    "    (Pipeline()\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=50)\n",
    "     >> ChooseBest(1))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a nice improvement. That's enough to get into the top 1% in the Kaggle Titatic demo competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting feature selection\n",
    "\n",
    "AutoML also allows you to select feature that perform well for the most models in the model space. Features that have geather importance in multiple models will have geather weight.\n",
    "\n",
    "\n",
    "First $N$ features are selected from the following well-ordered set:\n",
    "\n",
    "$\\mathbb{F} = softmax(\\mathbb{I}) \\circ \\mathbb{S}$,\n",
    "\n",
    "where \n",
    "* $\\mathbb{I} \\in \\mathbb{R}$ represents a model-specific feature score set\n",
    "* $\\mathbb{S} \\in \\mathbb{R}$ is a set of model scores according to some scoring function $s(x, m): \\mathbb{M} \\rightarrow \\mathbb{R} $ ($x$ is a dataset, $m$ is a model, $\\mathbb{M}$ is a model space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 9, new feature number - 10\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005646 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006252 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.37it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5359797138250997,\n",
      "       colsample_bytree=0.5942541970575874, gamma=1.631835074480241,\n",
      "       learning_rate=0.009892118048061794, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=27, missing=None, n_estimators=1000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.22749494361466274, reg_lambda=3.958250738618746,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.6852440733647458) - 0.7811447811447811\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.4881858318123985,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=19,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=512, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.8193041526374859\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.21it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #2\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:225: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005752 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005885 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.55it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.950075952313379,\n",
      "       colsample_bytree=0.9181038725274204, gamma=0.32168901060676525,\n",
      "       learning_rate=0.06464524162456323, max_delta_step=0, max_depth=9,\n",
      "       min_child_weight=2, missing=None, n_estimators=1600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0005663507943633604, reg_lambda=1.6739793070611109,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.9570228118172577) - 0.7957351290684623\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=7, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=125, n_jobs=1,\n",
      "            oob_score=False, random_state=4, verbose=False,\n",
      "            warm_start=False) - 0.8170594837261503\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.50it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #3\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:225: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004979 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004972 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.06it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.504432622775608,\n",
      "       colsample_bytree=0.7202245142570318, gamma=0.20683076627504385,\n",
      "       learning_rate=0.0010657467119514212, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=21, missing=None, n_estimators=4600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.21585666149318125, reg_lambda=2.004014067527034,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.7489857658590777) - 0.7811447811447811\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=301, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.7833894500561168\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.67it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #4\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004354 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006353 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:03<00:02,  1.05s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9304829927383204,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=128, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.7923681257014591\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.957296886641382,\n",
      "       colsample_bytree=0.9177387636730693, gamma=1.990848626800546,\n",
      "       learning_rate=0.031907763962176934, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=5, missing=None, n_estimators=3200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.48938375034078896, reg_lambda=2.59199678236793,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.5853028667253031) - 0.8148148148148149\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.56it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #5\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004788 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004938 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:07<00:04,  2.48s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9766777922667779,\n",
      "       colsample_bytree=0.6751157351817353, gamma=0.0001472417681630567,\n",
      "       learning_rate=0.004606873432225696, max_delta_step=0, max_depth=1,\n",
      "       min_child_weight=21, missing=None, n_estimators=3600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.2073035789361326, reg_lambda=2.877196934992784,\n",
      "       scale_pos_weight=1, seed=3, silent=True,\n",
      "       subsample=0.9859209655636547) - 0.7609427609427609\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.08430837535410451,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=2147, n_jobs=1, oob_score=False, random_state=3,\n",
      "            verbose=False, warm_start=False) - 0.8002244668911335\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.54s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #6\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004148 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004844 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.09it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.5509599272513994,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=6,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=13, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.8035914702581369\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.7424701270094602,\n",
      "       colsample_bytree=0.9224133127995351, gamma=0.00015269827328326716,\n",
      "       learning_rate=0.00018255246399316024, max_delta_step=0, max_depth=9,\n",
      "       min_child_weight=1, missing=None, n_estimators=3000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.028349178523440776, reg_lambda=1.7926163107605046,\n",
      "       scale_pos_weight=1, seed=0, silent=True,\n",
      "       subsample=0.8882391754431802) - 0.8170594837261506\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.58it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #7\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004088 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.008186 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.42it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6476089636729929,\n",
      "       colsample_bytree=0.9589800801668391, gamma=3.407681362306248e-05,\n",
      "       learning_rate=0.00018142539599349475, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=16, missing=None, n_estimators=3800, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.01628060180033915, reg_lambda=1.4122534693582964,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.7524559419818665) - 0.7890011223344557\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.29998045671541573,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=28, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.8237934904601572\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.21it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #8\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004580 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006675 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.42it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5652275484891014,\n",
      "       colsample_bytree=0.9452295277541942, gamma=0.005510077150465069,\n",
      "       learning_rate=0.08314308593411361, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=4, missing=None, n_estimators=1000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.008425287487572302, reg_lambda=1.14627315931722,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.8585419211933705) - 0.8047138047138048\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=578, n_jobs=1,\n",
      "            oob_score=False, random_state=2, verbose=False,\n",
      "            warm_start=False) - 0.8271604938271605\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.23it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #9\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004771 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004995 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:05<00:03,  1.75s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.8951587384939784,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=621, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.7957351290684626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6594225662844719,\n",
      "       colsample_bytree=0.9013811609771991, gamma=6.635964665856893e-05,\n",
      "       learning_rate=0.04852170881363913, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=2, missing=None, n_estimators=5200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.06968797181843013, reg_lambda=3.9882227684111107,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.6976318922100277) - 0.7968574635241302\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.13s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #10\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004343 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006968 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.31it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.8061487723343945,\n",
      "       colsample_bytree=0.6195949444257303, gamma=0.04942264134750671,\n",
      "       learning_rate=0.012125618462372366, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=86, missing=None, n_estimators=4600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.14867529760727508, reg_lambda=1.1448927161994087,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.7796082613122436) - 0.6161616161616161\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.45591356381931547,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=31, n_jobs=1, oob_score=False, random_state=2,\n",
      "            verbose=False, warm_start=False) - 0.7856341189674523\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.74it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #11\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004309 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004979 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.22it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=17, n_jobs=1,\n",
      "            oob_score=False, random_state=2, verbose=False,\n",
      "            warm_start=False) - 0.7912457912457912\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6270735828444567,\n",
      "       colsample_bytree=0.762042328010327, gamma=0.09077832038132347,\n",
      "       learning_rate=0.009746989958069857, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=6, missing=None, n_estimators=4200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0077302631222742465, reg_lambda=3.379495296448089,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.8505528970360272) - 0.8170594837261503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.88it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #12\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003961 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005344 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.35it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.8560884445862378,\n",
      "       colsample_bytree=0.7511476739689602, gamma=0.028870897341160533,\n",
      "       learning_rate=8.386185385832479e-05, max_delta_step=0, max_depth=2,\n",
      "       min_child_weight=82, missing=None, n_estimators=2200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0007940667747126813, reg_lambda=2.5052795682032656,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.8850515219989654) - 0.6161616161616161\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.917166109974044,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=61, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.7923681257014591\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.77it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #13\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005983 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.010634 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.66s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9103975643700264,\n",
      "       colsample_bytree=0.8928766341758281, gamma=0.0007997830765325932,\n",
      "       learning_rate=7.00133956541783e-05, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=23, missing=None, n_estimators=3800, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.12565218832469163, reg_lambda=2.5268371886756937,\n",
      "       scale_pos_weight=1, seed=3, silent=True,\n",
      "       subsample=0.9468229557272191) - 0.7934904601571269\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.17533788248939564,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1214, n_jobs=1, oob_score=False, random_state=1,\n",
      "            verbose=False, warm_start=False) - 0.8260381593714926\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #14\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:225: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007004 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005980 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.17it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.7688612356801597,\n",
      "       colsample_bytree=0.5994373226345181, gamma=0.040307877359135734,\n",
      "       learning_rate=0.0002848506355065094, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=13, missing=None, n_estimators=5400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.16066861507051033, reg_lambda=1.4973741938701945,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.8703775566168087) - 0.7957351290684623\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features=0.5346961535260688,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=192, n_jobs=1, oob_score=False, random_state=3,\n",
      "            verbose=False, warm_start=False) - 0.8271604938271605\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.80it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #15\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:225: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003778 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004735 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:00<00:00,  3.44it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5945063035102567,\n",
      "       colsample_bytree=0.800386207340179, gamma=0.4086330580091546,\n",
      "       learning_rate=0.11756031779551412, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=13, missing=None, n_estimators=1200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.6894398220598004, reg_lambda=1.1204072399916658,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.5648680767528451) - 0.7867564534231201\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.7712544138892193,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=18,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=41, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.8159371492704827\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.50it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #16\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004936 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005573 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.81it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9255274752187577,\n",
      "       colsample_bytree=0.8250173802381527, gamma=0.00022540216701266063,\n",
      "       learning_rate=0.0003798519167829367, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=11, missing=None, n_estimators=2600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0066542800038952585, reg_lambda=1.6469145119718094,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.6450034238043844) - 0.8069584736251403\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.0287079366226326,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=44, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.8114478114478114\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.82it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #17\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003671 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004826 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:09<00:06,  3.13s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=2388, n_jobs=1,\n",
      "            oob_score=False, random_state=3, verbose=False,\n",
      "            warm_start=False) - 0.809203142536476\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6636152168331797,\n",
      "       colsample_bytree=0.8757038094415219, gamma=6.325192481088378e-06,\n",
      "       learning_rate=0.00018917713204832736, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=4, missing=None, n_estimators=5400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.03876999626005913, reg_lambda=2.8114260074021447,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.5441555214578354) - 0.8204264870931538\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.97s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #18\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006143 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005168 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.73it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5597680631228583,\n",
      "       colsample_bytree=0.5098093726538968, gamma=0.0003121574229452646,\n",
      "       learning_rate=0.0015601472416514842, max_delta_step=0, max_depth=2,\n",
      "       min_child_weight=22, missing=None, n_estimators=4000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.001479091294488202, reg_lambda=1.491791366793895,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.7164881333462603) - 0.7991021324354658\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=168, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.8080808080808081\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.73it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #19\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004014 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004152 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:03<00:02,  1.04s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.8215711670755499,\n",
      "       colsample_bytree=0.8956450283957904, gamma=0.06304515876778904,\n",
      "       learning_rate=0.003477826639103132, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=10, missing=None, n_estimators=3200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.9583424876780158, reg_lambda=2.7985845647252594,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.8529840278459764) - 0.7957351290684623\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=436, n_jobs=1,\n",
      "            oob_score=False, random_state=4, verbose=False,\n",
      "            warm_start=False) - 0.809203142536476\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.53it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #20\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x1109336d8>, 'max_features': <hyperopt.pyll.base.Apply object at 0x1109533c8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110953dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110953160>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x11094fac8>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x11094f780>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003455 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11094f978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11094f668>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11094f470>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11094f2b0>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11058f780>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x10ff780b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11098d320>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11098d7f0>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11098d160>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11098dc18>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x11098d5c0>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004668 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.41it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.7016878524276637,\n",
      "       colsample_bytree=0.7922144606587491, gamma=0.0032404982090533034,\n",
      "       learning_rate=0.04027621911291763, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=15, missing=None, n_estimators=4400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.007732278314396481, reg_lambda=1.380834066859853,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.952372823845345) - 0.7766554433221099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.024374377716020135,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=87, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.8103254769921436\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> 0\n",
      "XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.7016878524276637,\n",
      "       colsample_bytree=0.7922144606587491, gamma=0.0032404982090533034,\n",
      "       learning_rate=0.04027621911291763, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=15, missing=None, n_estimators=4400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.007732278314396481, reg_lambda=1.380834066859853,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.952372823845345) 0.7766554433221099\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.024374377716020135,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=87, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) 0.8103254769921436\n",
      "(891, 5)\n",
      "Selected features:\n",
      "FamilySize_+_Sex_+_Pclass_-_Age_*_Sex_-_Age_*_Sex\n",
      "FamilySize_+_Sex_+_Pclass_-_Age_*_Sex_-_Age_*_Sex_+_FamilySize_+_Sex_+_Pclass_-_Age_*_Sex\n",
      "Sex_+_Pclass_+_Age_*_Sex\n",
      "FamilySize_+_Sex_+_Pclass_-_Age_*_Sex_-_Age_*_Sex_+_FamilySize_+_Sex_+_Pclass_-_Age_*_Sex_*_Sex_+_Pclass_+_Age_*_Sex\n",
      "FamilySize_+_Sex_+_Pclass_-_Age_*_Sex_-_Age_*_Sex_+_FamilySize_+_Sex_+_Pclass_-_Age_*_Sex_*_Sex_+_Pclass_+_Age_*_Sex_+_FamilySize_+_Sex_+_Pclass_-_Age_*_Sex_-_Age_*_Sex\n"
     ]
    }
   ],
   "source": [
    "from automl.hyperparam.optimization import Hyperopt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from automl.feature.selector import VotingFeatureSelector\n",
    "from automl.feature.generators import FormulaFeatureGenerator\n",
    "\n",
    "model_list = [\n",
    "      (RandomForestClassifier, random_forest_hp_space()),\n",
    "      (LogisticRegression, {}),\n",
    "      (XGBClassifier, xgboost_hp_space())\n",
    "  ]\n",
    "\n",
    "dataset = preprocess_data(data)\n",
    "\n",
    "context, pipeline_data = LocalExecutor(dataset, epochs=20) << \\\n",
    "    (Pipeline()\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     >> FormulaFeatureGenerator()\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=1)\n",
    "     >> ChooseBest(4, by_largest_score=False)\n",
    "     >> VotingFeatureSelector(feature_to_select=5, reverse_score=True))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)\n",
    "\n",
    "print(f\"Selected features:\")\n",
    "for col in pipeline_data.dataset.columns:\n",
    "    print(f\"{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 features were selected from an initial set on 9.\n",
    "\n",
    "# Reproducable preprocessing\n",
    "After you're done with AutoML model search it may be useful to reproduce resulting feature generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.,   6.,   6.,  36.,  38.],\n",
       "       [  3.,   6.,   1.,   6.,   9.],\n",
       "       [  4.,   8.,   3.,  24.,  28.],\n",
       "       ..., \n",
       "       [  7.,  14.,   3.,  42.,  49.],\n",
       "       [ -3.,  -3.,   5., -15., -18.],\n",
       "       [ -1.,   1.,   7.,   7.,   6.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from automl.feature.generators import Preprocessing\n",
    "\n",
    "original_dataset = preprocess_data(data)\n",
    "\n",
    "# Let's recreate all features useful features found in AutoML Pipeline\n",
    "preprocessing = Preprocessing()\n",
    "final_data = preprocessing.reproduce(pipeline_data.dataset, original_dataset)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
