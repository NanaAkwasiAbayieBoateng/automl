{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onepanel AutoML 0.1a - Kaggle Dataset Example\n",
    "\n",
    "Here we use AutoML to solve a classification task on a classic [Titanic](https://www.kaggle.com/c/titanic) dataset from Kaggle. First, let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived Pclass  \\\n",
       "0            1         0      3   \n",
       "1            2         1      1   \n",
       "2            3         1      3   \n",
       "3            4         1      1   \n",
       "4            5         0      3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/train.csv', parse_dates=[2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdubovikov/miniconda3/envs/automl-pip/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "# AutoML uses Python's logging module\n",
    "import logging\n",
    "\n",
    "# Various sklearn models and metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# AutoML Clasees\n",
    "from automl.pipeline import LocalExecutor, Pipeline, PipelineStep, PipelineData\n",
    "from automl.data.dataset import Dataset\n",
    "from automl.model import ModelSpace, CV, Validate, ChooseBest\n",
    "from automl.hyperparam.templates import (random_forest_hp_space, \n",
    "                                         knn_hp_space, svc_kernel_hp_space, \n",
    "                                         grad_boosting_hp_space, \n",
    "                                         xgboost_hp_space)\n",
    "from automl.feature.generators import FormulaFeatureGenerator, PolynomialGenerator\n",
    "from automl.feature.selector import FeatureSelector\n",
    "from automl.hyperparam.hyperopt import Hyperopt\n",
    "from automl.combinators import RandomChoice\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how automated our process is, data still may need some preprocessing. Also, doing good old feature engenering can help by a lot. We skip exploratory data analysis and feature engeneering stages for brevity. If you are interested, we suggest looking up some examples at contest's [kernels](https://www.kaggle.com/c/titanic/kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data and create AutoML Dataset\"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    result = df.copy()\n",
    "    \n",
    "    # drop columns we won't be using\n",
    "    result.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "    \n",
    "    # transform Sex column into numeric categories\n",
    "    result['Sex'] = encoder.fit_transform(result['Sex'])\n",
    "    \n",
    "    # do the same with Embarked column\n",
    "    result['Embarked'] = encoder.fit_transform(result['Embarked'].astype(str))\n",
    "    \n",
    "    # Replace missing Ages with median value and \"pack\"\n",
    "    # Age into 10 equal-sized bins. For example, all \n",
    "    # ages from 0-10 will be packed into bin 0.\n",
    "    result['Age'].fillna(result['Age'].median(), inplace=True)\n",
    "    result['Age'] = pd.cut(result['Age'], 10, labels=range(0,10)).astype(int)\n",
    "    \n",
    "    # Pack Fare into 10 bins\n",
    "    result['Fare'] = pd.cut(result['Fare'], 10, labels=range(0,10)).astype(int)\n",
    "    \n",
    "    # transform Pclass type to int\n",
    "    result['Pclass'] = result['Pclass'].astype(int)\n",
    "    \n",
    "    # add some useful predictive features that may came \n",
    "    # up to mind during data analysis\n",
    "    result['FamilySize'] = result['SibSp'] + result['Parch'] + 1\n",
    "    result['IsAlone'] = 0\n",
    "    result.loc[result['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    return Dataset(result.drop(['Survived'], axis=1),\n",
    "                   result['Survived'])\n",
    "\n",
    "dataset = preprocess_data(data)\n",
    "print(f\"Features: {dataset.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit simple XGBoost model with default parameters and see how it scores on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81144781144781142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf = XGBClassifier()\n",
    "np.mean(cross_val_score(rf, dataset.data, dataset.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 81% accuracy with the defaults. Let's go on to AutoML Pipelines and see if we can improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'SklearnFeatureGenerator'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x10ed188d0>, 'max_features': <hyperopt.pyll.base.Apply object at 0x112000c88>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x112000dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x112000550>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x112000438>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x1120002b0>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004746 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003951 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003543 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.194164\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003580 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.194164\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003490 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003609 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004022 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003837 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004154 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004677 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004263 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004985 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004193 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004795 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003720 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004772 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003691 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005135 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005489 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.175084\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003590 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.175084\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x10cbc0470>, 'weights': <hyperopt.pyll.base.Apply object at 0x1120a4ac8>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001298 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002235 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.254770\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002211 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002827 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003048 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002327 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002717 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002274 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003464 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002171 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002704 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002292 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002662 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002542 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002701 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002079 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002267 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002906 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002008 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.207632\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002898 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.207632\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11208da20>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11208d828>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11208d668>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11208d588>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11208d278>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x11208d4a8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11208d128>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11208df60>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11208dc50>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11208da90>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x111ffd978>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004062 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004764 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.213244\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004522 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.199776\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004607 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.199776\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004737 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.186308\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004561 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004571 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004616 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005660 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004721 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004724 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004514 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005353 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005894 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005794 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004403 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004877 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004637 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006919 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.176207\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004643 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.176207\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [01:37<01:05, 32.55s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.7183832499152458,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=25,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1225, n_jobs=1, oob_score=False, random_state=3,\n",
      "            verbose=False, warm_start=False) - 0.8249158249158249\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6554085263013064,\n",
      "       colsample_bytree=0.6413419070520245, gamma=2.559597691330678,\n",
      "       learning_rate=0.001337062204651526, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=1, missing=None, n_estimators=2600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0001224306470678317, reg_lambda=3.012470316980132,\n",
      "       scale_pos_weight=1, seed=3, silent=True,\n",
      "       subsample=0.7464582402145266) - 0.8237934904601572\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=36, p=2,\n",
      "           weights='distance') - 0.7923681257014591\n",
      "LocalExecutor - INFO - Running step 'FeatureSelector'\n",
      "FeatureSelector - INFO - Removing 20 features for model RandomForestClassifier\n",
      "100%|██████████| 5/5 [01:37<00:00, 19.56s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #2\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'SklearnFeatureGenerator'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x10ed188d0>, 'max_features': <hyperopt.pyll.base.Apply object at 0x112000c88>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x112000dd8>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x112000550>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x112000438>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x1120002b0>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003823 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003654 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.200898\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004625 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004979 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.186308\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004614 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.186308\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003523 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.186308\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005313 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.186308\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004824 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005026 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003741 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004220 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004072 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004393 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005633 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003544 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003990 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004735 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003833 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003628 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004434 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.181818\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x10cbc0470>, 'weights': <hyperopt.pyll.base.Apply object at 0x1120a4ac8>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001457 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002541 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.253648\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002700 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.252525\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004313 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002969 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002363 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002429 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004705 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002617 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.240180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - tpe_transform took 0.004364 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.210999\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002823 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.210999\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005069 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.210999\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002273 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003099 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002829 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002238 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003078 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002359 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.208754\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003629 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.208754\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002550 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.208754\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x11208da20>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x11208d828>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x11208d668>, 'gamma': <hyperopt.pyll.base.Apply object at 0x11208d588>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x11208d278>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x11208d4a8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x11208d128>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x11208df60>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x11208dc50>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x11208da90>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x111ffd978>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004752 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005935 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.213244\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005964 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.213244\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004537 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.203143\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005021 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005003 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005464 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005859 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004606 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004571 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004622 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005178 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005560 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005878 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005885 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006081 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005443 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006620 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006029 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.182941\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006039 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.182941\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [09:04<06:02, 181.43s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=11, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=2935, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=False,\n",
      "            warm_start=False) - 0.8181818181818182\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.8714378354989234,\n",
      "       colsample_bytree=0.6233223238530081, gamma=3.868832461192523,\n",
      "       learning_rate=0.17910326554945424, max_delta_step=0, max_depth=1,\n",
      "       min_child_weight=26, missing=None, n_estimators=2000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0001231647878815767, reg_lambda=2.5371027020895482,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.7221340829175613) - 0.8170594837261503\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=40, p=2,\n",
      "           weights='distance') - 0.7912457912457912\n",
      "LocalExecutor - INFO - Running step 'FeatureSelector'\n",
      "FeatureSelector - INFO - Removing 20 features for model RandomForestClassifier\n",
      "100%|██████████| 5/5 [09:04<00:00, 108.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=11, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=2935, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=False,\n",
      "            warm_start=False) 0.8181818181818182\n",
      "(891, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, we define our ModelSpace. ModelSpace is initialized by a list of tuples.\n",
    "# First element of each tuple should be an sklearn-like estimator with fit method\n",
    "# The second one is model parameter dictionary. Here we do not define parameters \n",
    "# explicitly, but use hyperparameter templates from AutoML. Those templates can be\n",
    "# used later by Hyperopt step to find best model parameters automatically\n",
    "model_list = [\n",
    "      (RandomForestClassifier, random_forest_hp_space()),\n",
    "      (KNeighborsClassifier, knn_hp_space(lambda key: key)),\n",
    "      (XGBClassifier, xgboost_hp_space())\n",
    "  ]\n",
    "\n",
    "\n",
    "# Create executor, initialize it with our classification dataset \n",
    "# and set total number of epochs to 2 (the pipeline will be run two times in a row).\n",
    "# We can load any pipeline into executor using << operator like below:\n",
    "context, pipeline_data = LocalExecutor(dataset, epochs=2) << \\\n",
    "    (Pipeline() # Here we define the pipeline. Steps can be added to pipeline using >> operator\n",
    "     # First we define our ModelSpace. We wrap it with PipelineStep class \n",
    "     # and set initializer=True so that ModelSpace step will be run only at the first epoch\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     # But we are not obliged to wrap all steps with PipelineStep.\n",
    "     # This will be done automatically if we do not need to set any special parameters \n",
    "     # We use PolynomialGenerator to create polynomial combinations of the features from the dataset\n",
    "     >> PolynomialGenerator()\n",
    "     # Next we use Hyperopt to find the best combination of hyperparameters for each model\n",
    "     # We use test set validation with accuracy metric as a score function.\n",
    "     # CV could be used instead of Validate to perform cross-validation\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=20)\n",
    "     # Then we choose the best performing model we found\n",
    "     >> ChooseBest(1)\n",
    "     # And select 10 best features\n",
    "     >> FeatureSelector(20))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have reached better accuracy compared to default XGBoost plain dataset. However, first pipeline we launched had a good job of generating various features for our dataset, but it was not really created for searching the best model. Now let's create a pipeline which will search for the best model on a fixed dataset.\n",
    "\n",
    "Please note that increasing `max_evals` parameter for `Hyperopt` can lead to finding better model parameters, but we use modest values here for demonstation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x110afd710>, 'max_features': <hyperopt.pyll.base.Apply object at 0x110b129e8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12278>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110b12630>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x110b126a0>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x110ae5f60>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004085 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004677 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005231 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004224 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004392 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004576 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005282 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004930 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004922 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004776 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004233 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004856 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004926 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005101 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003870 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004319 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004134 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004484 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004554 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005350 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004570 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004733 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005655 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004903 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004796 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004253 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004385 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004349 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004464 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005702 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005470 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004899 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003714 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003747 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004631 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005213 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004022 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004254 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004253 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006715 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007372 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004691 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003693 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004531 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004095 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004867 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005945 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005553 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004377 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003906 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.195286\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x110b127b8>, 'weights': <hyperopt.pyll.base.Apply object at 0x110b129b0>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001962 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005780 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.391695\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002883 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.305275\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003809 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002699 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002689 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002789 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002775 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002217 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002209 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002209 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003297 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002305 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002389 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002245 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002239 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002438 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003408 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002822 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003121 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002424 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002224 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002140 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002584 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002756 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001987 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001784 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002093 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003823 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002134 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002509 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002629 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002528 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002819 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002263 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002270 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002897 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003463 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002175 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003521 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002997 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002758 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003361 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002474 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003699 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002404 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002361 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002206 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002219 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003480 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.202020\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12cf8>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x110b3b128>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x110b3b320>, 'gamma': <hyperopt.pyll.base.Apply object at 0x110b3b4a8>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x110b3b668>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x110b3b7b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x110b3b908>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x110b3ba58>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x110b3bbe0>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x110b3bd68>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x110b3be10>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004497 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004549 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005759 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004673 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004891 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005388 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005311 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004989 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005397 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005384 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005488 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004770 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.208754\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004906 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.206510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - tpe_transform took 0.005166 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005136 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004515 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004597 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006371 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006206 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005871 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005204 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005182 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006584 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005502 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004828 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004749 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004696 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004766 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007035 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005002 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005482 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004729 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.008283 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004530 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005204 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004767 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006708 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004693 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004760 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005397 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004699 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005162 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004762 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005360 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005784 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005462 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007726 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004672 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005405 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004617 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.204265\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 67%|██████▋   | 2/3 [03:20<01:40, 100.11s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=77, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.8047138047138048\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform') - 0.797979797979798\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9439801428101403,\n",
      "       colsample_bytree=0.5542595913412373, gamma=4.983853559643964,\n",
      "       learning_rate=0.00983614007205386, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.07602348522584429, reg_lambda=1.245497994618211,\n",
      "       scale_pos_weight=1, seed=3, silent=True,\n",
      "       subsample=0.6872993176982336) - 0.7957351290684623\n",
      "100%|██████████| 3/3 [03:20<00:00, 66.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=77, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) 0.8047138047138048\n",
      "(891, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context, pipeline_data = LocalExecutor(pipeline_data.dataset, epochs=1) << \\\n",
    "    (Pipeline()\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=50)\n",
    "     >> ChooseBest(1))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a nice improvement. That's enough to get into the top 1% in the Kaggle Titatic demo competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting feature selection\n",
    "\n",
    "AutoML also allows you to select feature that perform well for the most models in the model space. Features that have geather importance in multiple models will have geather weight.\n",
    "\n",
    "\n",
    "First $N$ features are selected from the following well-ordered set:\n",
    "\n",
    "$\\mathbb{F} = softmax(\\mathbb{I}) \\circ \\mathbb{S}$,\n",
    "\n",
    "where \n",
    "* $\\mathbb{I} \\in \\mathbb{R}$ represents a model-specific feature score set\n",
    "* $\\mathbb{S} \\in \\mathbb{R}$ is a set of model scores according to some scoring function $s(x, m): \\mathbb{M} \\rightarrow \\mathbb{R} $ ($x$ is a dataset, $m$ is a model, $\\mathbb{M}$ is a model space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 9, new feature number - 10\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004159 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005678 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.58it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=117, n_jobs=1,\n",
      "            oob_score=False, random_state=4, verbose=False,\n",
      "            warm_start=False) - 0.787878787878788\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6557152657093197,\n",
      "       colsample_bytree=0.5291659799010566, gamma=0.019008022234991005,\n",
      "       learning_rate=0.02441629193420657, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=14, missing=None, n_estimators=2800, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.07921019834361051, reg_lambda=2.0754755879667246,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.9185769716412922) - 0.797979797979798\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #2\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004346 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004841 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.00it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5567658703417129,\n",
      "       colsample_bytree=0.71065689587635, gamma=1.7507998714961317,\n",
      "       learning_rate=0.01369989566135996, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=1, missing=None, n_estimators=4600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.06095967145084272, reg_lambda=1.5174049281019484,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.6641357149317682) - 0.8181818181818182\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=84, n_jobs=1,\n",
      "            oob_score=False, random_state=3, verbose=False,\n",
      "            warm_start=False) - 0.8226711560044894\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.62it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #3\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004786 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004659 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.33it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.19609945609078838,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=32,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=39, n_jobs=1, oob_score=False, random_state=1,\n",
      "            verbose=False, warm_start=False) - 0.7721661054994389\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.991633911388094,\n",
      "       colsample_bytree=0.7706739830613925, gamma=0.0033476484199192473,\n",
      "       learning_rate=0.11009337043823757, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=16, missing=None, n_estimators=5200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.042971012610138824, reg_lambda=2.4928603671383605,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.6325591414868514) - 0.809203142536476\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #4\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004323 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004905 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.20it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9031289727144572,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=462, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.7856341189674523\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.7809384263427759,\n",
      "       colsample_bytree=0.7615918271599358, gamma=0.4890401277357081,\n",
      "       learning_rate=0.0007853777116285624, max_delta_step=0, max_depth=1,\n",
      "       min_child_weight=7, missing=None, n_estimators=3600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.014980647799128263, reg_lambda=2.5844683136792588,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.9776260025016051) - 0.7867564534231201\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.91it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #5\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004061 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004019 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.65s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.4580048724069996,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1557, n_jobs=1, oob_score=False, random_state=2,\n",
      "            verbose=False, warm_start=False) - 0.8013468013468014\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6892287123724905,\n",
      "       colsample_bytree=0.5907905649273785, gamma=0.029725549388958335,\n",
      "       learning_rate=0.0031939248929221038, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=2200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.2953408947557093, reg_lambda=3.8753199488806347,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.8973058042055494) - 0.8159371492704826\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #6\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004345 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005583 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.56it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.10143105670993857,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=187, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.8058361391694725\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5324667083441694,\n",
      "       colsample_bytree=0.9499269297534821, gamma=0.06528538144615603,\n",
      "       learning_rate=0.0011971432141082779, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=2, missing=None, n_estimators=2200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.032360291204190865, reg_lambda=3.7222310790579356,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.7458924174714084) - 0.8193041526374859\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.37it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #7\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004263 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004477 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:05<00:03,  1.96s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features=0.9557966868455473,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=2229, n_jobs=1, oob_score=False, random_state=3,\n",
      "            verbose=False, warm_start=False) - 0.8215488215488215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6083115341159874,\n",
      "       colsample_bytree=0.9958321234616289, gamma=0.008418514642201332,\n",
      "       learning_rate=0.015280550283312879, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=6, missing=None, n_estimators=400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.00018721999851741222, reg_lambda=1.1821199127464457,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.8433763925951976) - 0.8226711560044894\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.23s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #8\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003980 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004685 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:06<00:04,  2.06s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.775248346775853,\n",
      "       colsample_bytree=0.9606298592428786, gamma=0.4492828200007534,\n",
      "       learning_rate=0.0022839120702587328, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=60, missing=None, n_estimators=1800, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.21356631720697486, reg_lambda=2.1361264663480997,\n",
      "       scale_pos_weight=1, seed=0, silent=True,\n",
      "       subsample=0.6967032736058549) - 0.6161616161616161\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.7182278901667156,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=2053, n_jobs=1, oob_score=False, random_state=2,\n",
      "            verbose=False, warm_start=False) - 0.8058361391694726\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #9\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004190 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004860 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.02it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.5248405802573695,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=57, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.8002244668911335\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.631608635858899,\n",
      "       colsample_bytree=0.7497224088949708, gamma=0.18935923128535168,\n",
      "       learning_rate=0.26651452216425053, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=3, missing=None, n_estimators=5000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0006716460539486554, reg_lambda=3.123069733752024,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.7682419488670225) - 0.8058361391694726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.58it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #10\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004206 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005806 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.15it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9431988093225678,\n",
      "       colsample_bytree=0.609005040542234, gamma=8.758681525955588e-05,\n",
      "       learning_rate=0.03487037683466624, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=7, missing=None, n_estimators=6000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.028496934089931727, reg_lambda=2.2452204153285344,\n",
      "       scale_pos_weight=1, seed=0, silent=True,\n",
      "       subsample=0.5753648899728967) - 0.7968574635241302\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.8013468013468014\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.78it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #11\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004436 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004812 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.61it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=39, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=259, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.7934904601571269\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.8226208711060444,\n",
      "       colsample_bytree=0.8000560314698477, gamma=0.09463999952980734,\n",
      "       learning_rate=9.18670281900735e-05, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=13, missing=None, n_estimators=2800, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.04978463890304472, reg_lambda=2.5328769143221446,\n",
      "       scale_pos_weight=1, seed=0, silent=True,\n",
      "       subsample=0.5223384459470217) - 0.7934904601571269\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #12\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004273 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005937 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:07<00:04,  2.48s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=0.23937639619045736,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=11,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=2791, n_jobs=1, oob_score=False, random_state=3,\n",
      "            verbose=False, warm_start=False) - 0.7934904601571269\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.7435020618771497,\n",
      "       colsample_bytree=0.6582418147355245, gamma=9.797231314022556e-05,\n",
      "       learning_rate=0.006092929838345647, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=28, missing=None, n_estimators=4400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.2674284894379938, reg_lambda=2.3148796398009934,\n",
      "       scale_pos_weight=1, seed=0, silent=True,\n",
      "       subsample=0.9127223036105494) - 0.7934904601571269\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.57s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #13\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004315 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005090 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:03<00:02,  1.05s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features=0.25950966074767656,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=56, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.7934904601571269\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6680338404701303,\n",
      "       colsample_bytree=0.9658574145889756, gamma=4.343693630815416,\n",
      "       learning_rate=0.002148964566335014, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=4, missing=None, n_estimators=5400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=1.4241152242915108e-05, reg_lambda=2.908644725498779,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.664969834969312) - 0.7934904601571269\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.55it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #14\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004321 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004559 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:05<00:03,  1.76s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5943250329330265,\n",
      "       colsample_bytree=0.9832618907928532, gamma=0.0008093923877755677,\n",
      "       learning_rate=0.0024804575018263944, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=6, missing=None, n_estimators=600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0012063352688217988, reg_lambda=1.2769070339444262,\n",
      "       scale_pos_weight=1, seed=3, silent=True,\n",
      "       subsample=0.6427141414844466) - 0.7934904601571269\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1929, n_jobs=1,\n",
      "            oob_score=False, random_state=4, verbose=False,\n",
      "            warm_start=False) - 0.8024691358024691\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.09s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #15\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004721 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007007 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:00<00:00,  3.66it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=28, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.7878787878787877\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.879094924321302,\n",
      "       colsample_bytree=0.767982602278392, gamma=2.967885568967822,\n",
      "       learning_rate=0.00010992733818605145, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=13, missing=None, n_estimators=1000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.062308749234305134, reg_lambda=2.660406289033197,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.927144120477179) - 0.7934904601571269\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.84it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #16\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004195 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005034 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.68it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features=0.1915193703167708,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=35,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=246, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=False, warm_start=False) - 0.7934904601571269\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.588861952782362,\n",
      "       colsample_bytree=0.7471013389401557, gamma=0.09048941821288047,\n",
      "       learning_rate=0.007168279934156349, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=36, missing=None, n_estimators=4400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.010991099243911279, reg_lambda=3.691227808066216,\n",
      "       scale_pos_weight=1, seed=1, silent=True,\n",
      "       subsample=0.8519599698442204) - 0.7934904601571269\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #17\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004203 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005232 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.48it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.5337465847803237,\n",
      "       colsample_bytree=0.7216322420096395, gamma=0.00016862644725415999,\n",
      "       learning_rate=2.1010028146164738e-05, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=17, missing=None, n_estimators=4800, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.3107845443069692, reg_lambda=1.785024432950131,\n",
      "       scale_pos_weight=1, seed=0, silent=True,\n",
      "       subsample=0.762747049103052) - 0.7934904601571269\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features=0.40605012518523054,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=107, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.8024691358024691\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.30it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #18\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004121 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004647 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.36it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.32984866125793166,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=7,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=447, n_jobs=1, oob_score=False, random_state=2,\n",
      "            verbose=False, warm_start=False) - 0.7867564534231201\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.6411443264170507,\n",
      "       colsample_bytree=0.714607443333956, gamma=0.0010914663424632753,\n",
      "       learning_rate=0.0012251494596873583, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=23, missing=None, n_estimators=4000, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.03191908368288765, reg_lambda=2.5671916084940034,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.5495818826712484) - 0.7934904601571269\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #19\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 6\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003851 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006312 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.35it/s]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.8706469606694027,\n",
      "       colsample_bytree=0.5718311082171652, gamma=1.3066838568978958e-05,\n",
      "       learning_rate=0.000513128788594507, max_delta_step=0, max_depth=1,\n",
      "       min_child_weight=99, missing=None, n_estimators=4400, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.00027112915501586245, reg_lambda=1.7688141474148276,\n",
      "       scale_pos_weight=1, seed=4, silent=True,\n",
      "       subsample=0.9439813420543406) - 0.6161616161616161\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=56, n_jobs=1,\n",
      "            oob_score=False, random_state=4, verbose=False,\n",
      "            warm_start=False) - 0.8024691358024691\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.79it/s]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #20\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "/Users/kdubovikov/Projects/work/OnePanel/auto-ml/automl/feature/generators.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return x / y, history, name\n",
      "FormulaFeatureGenerator - INFO - Generated new features. Old feature number - 5, new feature number - 5\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x11380c550>, 'max_features': <hyperopt.pyll.base.Apply object at 0x11380cc50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x11380cf60>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x1158120b8>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x115812208>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x115812320>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005326 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {}\n",
      "Hyperopt - WARNING - Skipping hyperopt step for model <class 'sklearn.linear_model.logistic.LogisticRegression'>. No parameter templats found\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x115812978>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x115812438>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x115812eb8>, 'gamma': <hyperopt.pyll.base.Apply object at 0x115812748>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x1158125f8>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x115812b00>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x115812e80>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x1157e5160>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x1157e52e8>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x1157e5470>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x1157e5518>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005192 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [00:03<00:02,  1.09s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - <class 'sklearn.linear_model.logistic.LogisticRegression'> - 0\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=242, n_jobs=1,\n",
      "            oob_score=False, random_state=4, verbose=False,\n",
      "            warm_start=False) - 0.7890011223344556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9409202069976053,\n",
      "       colsample_bytree=0.9178224275088251, gamma=0.0008887275353809392,\n",
      "       learning_rate=0.033426701248356304, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=4, missing=None, n_estimators=4600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.008681998079279135, reg_lambda=2.202762869564714,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.7584289020642088) - 0.7946127946127945\n",
      "LocalExecutor - INFO - Running step 'VotingFeatureSelector'\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> 0\n",
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=242, n_jobs=1,\n",
      "            oob_score=False, random_state=4, verbose=False,\n",
      "            warm_start=False) 0.7890011223344556\n",
      "XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9409202069976053,\n",
      "       colsample_bytree=0.9178224275088251, gamma=0.0008887275353809392,\n",
      "       learning_rate=0.033426701248356304, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=4, missing=None, n_estimators=4600, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.008681998079279135, reg_lambda=2.202762869564714,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.7584289020642088) 0.7946127946127945\n",
      "(891, 5)\n",
      "Selected features:\n",
      "Sex_*_Pclass_*_Age_/_Pclass_+_Sex_*_Pclass_*_Age_/_Pclass\n",
      "Age_/_Pclass_*_Sex_*_Pclass\n",
      "Age_/_Pclass_*_Sex_*_Pclass_*_Age_/_Pclass\n",
      "Age_/_Pclass_*_Sex_*_Pclass_+_Age_/_Pclass\n",
      "Sex_*_Pclass_*_Age_/_Pclass_+_Sex_*_Pclass_*_Age_/_Pclass_*_Age_/_Pclass_*_Sex_*_Pclass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from automl.hyperparam.hyperopt import Hyperopt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from automl.feature.selector import VotingFeatureSelector\n",
    "from automl.feature.generators import FormulaFeatureGenerator\n",
    "\n",
    "model_list = [\n",
    "      (RandomForestClassifier, random_forest_hp_space()),\n",
    "      (LogisticRegression, {}),\n",
    "      (XGBClassifier, xgboost_hp_space())\n",
    "  ]\n",
    "\n",
    "dataset = preprocess_data(data)\n",
    "\n",
    "context, pipeline_data = LocalExecutor(dataset, epochs=20) << \\\n",
    "    (Pipeline()\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     >> FormulaFeatureGenerator()\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=1)\n",
    "     >> ChooseBest(4, by_largest_score=False)\n",
    "     >> VotingFeatureSelector(feature_to_select=5, reverse_score=True))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)\n",
    "\n",
    "print(f\"Selected features:\")\n",
    "for col in pipeline_data.dataset.columns:\n",
    "    print(f\"{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 features were selected from an initial set on 9.\n",
    "\n",
    "# Reproducable preprocessing\n",
    "After you're done with AutoML model search it may be useful to reproduce resulting feature generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.        ,   2.        ,   1.33333337,   2.66666675,   8.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   4.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   1.        ,   0.        ],\n",
       "       ..., \n",
       "       [  0.        ,   0.        ,   0.        ,   1.        ,   0.        ],\n",
       "       [  6.        ,   3.        ,   9.        ,   6.        ,  18.        ],\n",
       "       [  6.        ,   3.        ,   3.        ,   4.        ,  18.        ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from automl.feature.generators import Preprocessing\n",
    "\n",
    "original_dataset = preprocess_data(data)\n",
    "\n",
    "# Let's recreate all features useful features found in AutoML Pipeline\n",
    "preprocessing = Preprocessing()\n",
    "final_data = preprocessing.reproduce(pipeline_data.dataset, original_dataset)\n",
    "final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
