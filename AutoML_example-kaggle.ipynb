{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onepanel AutoML 0.1a - Kaggle Dataset Example\n",
    "\n",
    "Here we use AutoML to solve a classification task on a classic [Titanic](https://www.kaggle.com/c/titanic) dataset from Kaggle. First, let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived Pclass  \\\n",
       "0            1         0      3   \n",
       "1            2         1      1   \n",
       "2            3         1      3   \n",
       "3            4         1      1   \n",
       "4            5         0      3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/train.csv', parse_dates=[2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdubovikov/miniconda3/envs/automl-pip/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "# AutoML uses Python's logging module\n",
    "import logging\n",
    "\n",
    "# Various sklearn models and metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# AutoML Clasees\n",
    "from automl.pipeline import LocalExecutor, Pipeline, PipelineStep, PipelineData\n",
    "from automl.data.dataset import Dataset\n",
    "from automl.model import ModelSpace, CV, Validate, ChooseBest\n",
    "from automl.hyperparam.templates import (random_forest_hp_space, \n",
    "                                         knn_hp_space, svc_kernel_hp_space, \n",
    "                                         grad_boosting_hp_space, \n",
    "                                         xgboost_hp_space)\n",
    "from automl.feature.generators import FormulaFeatureGenerator, PolynomialGenerator\n",
    "from automl.feature.selector import FeatureSelector\n",
    "from automl.hyperparam.hyperopt import Hyperopt\n",
    "from automl.combinators import RandomChoice\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how automated our process is, data still may need some preprocessing. Also, doing good old feature engenering can help by a lot. We skip exploratory data analysis and feature engeneering stages for brevity. If you are interested, we suggest looking up some examples at contest's [kernels](https://www.kaggle.com/c/titanic/kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data and create AutoML Dataset\"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    result = df.copy()\n",
    "    \n",
    "    # drop columns we won't be using\n",
    "    result.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "    \n",
    "    # transform Sex column into numeric categories\n",
    "    result['Sex'] = encoder.fit_transform(result['Sex'])\n",
    "    \n",
    "    # do the same with Embarked column\n",
    "    result['Embarked'] = encoder.fit_transform(result['Embarked'].astype(str))\n",
    "    \n",
    "    # Replace missing Ages with median value and \"pack\"\n",
    "    # Age into 10 equal-sized bins. For example, all \n",
    "    # ages from 0-10 will be packed into bin 0.\n",
    "    result['Age'].fillna(result['Age'].median(), inplace=True)\n",
    "    result['Age'] = pd.cut(result['Age'], 10, labels=range(0,10)).astype(int)\n",
    "    \n",
    "    # Pack Fare into 10 bins\n",
    "    result['Fare'] = pd.cut(result['Fare'], 10, labels=range(0,10)).astype(int)\n",
    "    \n",
    "    # transform Pclass type to int\n",
    "    result['Pclass'] = result['Pclass'].astype(int)\n",
    "    \n",
    "    # add some useful predictive features that may came \n",
    "    # up to mind during data analysis\n",
    "    result['FamilySize'] = result['SibSp'] + result['Parch'] + 1\n",
    "    result['IsAlone'] = 0\n",
    "    result.loc[result['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    return Dataset(result.drop(['Survived'], axis=1),\n",
    "                   result['Survived'])\n",
    "\n",
    "dataset = preprocess_data(data)\n",
    "print(f\"Features: {dataset.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit simple XGBoost model with default parameters and see how it scores on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81144781144781142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf = XGBClassifier()\n",
    "np.mean(cross_val_score(rf, dataset.data, dataset.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 81% accuracy with the defaults. Let's go on to AutoML Pipelines and see if we can improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'SklearnFeatureGenerator'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x110afd710>, 'max_features': <hyperopt.pyll.base.Apply object at 0x110b129e8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12278>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110b12630>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x110b126a0>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x110ae5f60>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004058 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004321 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.218855\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005335 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.208754\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004672 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004058 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004502 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003624 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003814 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004594 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003958 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003948 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003749 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004870 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004852 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004192 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.180696\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004245 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004394 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003608 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004563 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.178451\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003629 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.178451\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x110b127b8>, 'weights': <hyperopt.pyll.base.Apply object at 0x110b129b0>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001232 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001887 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.250281\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002566 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.250281\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001996 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.250281\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001916 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.250281\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003936 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.246914\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003186 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.246914\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001918 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.246914\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002229 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.246914\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001955 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.234568\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002177 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.234568\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002570 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.233446\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002533 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.233446\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002574 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.233446\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003098 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003181 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002081 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002037 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002360 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.232323\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002803 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.232323\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12cf8>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x110b3b128>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x110b3b320>, 'gamma': <hyperopt.pyll.base.Apply object at 0x110b3b4a8>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x110b3b668>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x110b3b7b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x110b3b908>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x110b3ba58>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x110b3bbe0>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x110b3bd68>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x110b3be10>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004363 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004487 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.383838\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004673 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.383838\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004547 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004801 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004654 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005991 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004611 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.187430\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004579 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006979 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005700 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005704 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006170 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005395 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004585 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005900 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004662 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004869 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004957 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.173962\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004544 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.173962\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [02:12<01:28, 44.12s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9906095381045703,\n",
      "       colsample_bytree=0.8208150475586677, gamma=7.13611003203062e-05,\n",
      "       learning_rate=0.0015263545965457127, max_delta_step=0, max_depth=1,\n",
      "       min_child_weight=2, missing=None, n_estimators=3200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.7070522889072268, reg_lambda=1.3215490853139205,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.9588818461609412) - 0.8260381593714926\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features=0.6222400414286431,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=43, n_jobs=1, oob_score=False, random_state=4,\n",
      "            verbose=False, warm_start=False) - 0.8215488215488215\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=23, p=2,\n",
      "           weights='distance') - 0.7676767676767676\n",
      "LocalExecutor - INFO - Running step 'FeatureSelector'\n",
      "FeatureSelector - INFO - Removing 55 features for model XGBClassifier\n",
      "100%|██████████| 5/5 [02:12<00:00, 26.48s/it]\n",
      "LocalExecutor - INFO - Starting AutoML Epoch #2\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "PipelineStep - INFO - Initializer step model space was already run, skipping\n",
      "LocalExecutor - INFO - Running step 'SklearnFeatureGenerator'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x110afd710>, 'max_features': <hyperopt.pyll.base.Apply object at 0x110b129e8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12278>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110b12630>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x110b126a0>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x110ae5f60>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003940 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004645 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004487 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004199 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004641 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004822 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003988 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004740 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003964 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004262 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004886 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004662 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004949 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004137 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004875 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003734 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005148 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004808 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003897 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.181818\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003853 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.181818\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x110b127b8>, 'weights': <hyperopt.pyll.base.Apply object at 0x110b129b0>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001932 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002221 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.242424\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002175 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.242424\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003224 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003524 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002147 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002219 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.240180\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002691 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002499 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.237935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - tpe_transform took 0.002109 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002368 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002799 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003096 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002734 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002305 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002319 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002870 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002142 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002437 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.237935\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002191 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.237935\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12cf8>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x110b3b128>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x110b3b320>, 'gamma': <hyperopt.pyll.base.Apply object at 0x110b3b4a8>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x110b3b668>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x110b3b7b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x110b3b908>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x110b3ba58>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x110b3bbe0>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x110b3bd68>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x110b3be10>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004651 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005592 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005443 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005920 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005527 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005217 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005632 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006455 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005437 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005500 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005403 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005830 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005827 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005337 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005454 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005094 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007209 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004569 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006137 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.188552\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004600 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.188552\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 60%|██████    | 3/5 [1:14:48<49:52, 1496.06s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=145, n_jobs=1,\n",
      "            oob_score=False, random_state=3, verbose=False,\n",
      "            warm_start=False) - 0.8181818181818182\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.718158325208187,\n",
      "       colsample_bytree=0.7451251873015641, gamma=3.852030611320689,\n",
      "       learning_rate=0.05459693762105079, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=6, missing=None, n_estimators=3200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.004166699172078985, reg_lambda=1.0182957312153182,\n",
      "       scale_pos_weight=1, seed=2, silent=True,\n",
      "       subsample=0.9515860269728631) - 0.8114478114478114\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=20, p=2,\n",
      "           weights='distance') - 0.7620650953984288\n",
      "LocalExecutor - INFO - Running step 'FeatureSelector'\n",
      "FeatureSelector - INFO - Removing 20 features for model RandomForestClassifier\n",
      "100%|██████████| 5/5 [1:14:48<00:00, 897.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=145, n_jobs=1,\n",
      "            oob_score=False, random_state=3, verbose=False,\n",
      "            warm_start=False) 0.8181818181818182\n",
      "(891, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, we define our ModelSpace. ModelSpace is initialized by a list of tuples.\n",
    "# First element of each tuple should be an sklearn-like estimator with fit method\n",
    "# The second one is model parameter dictionary. Here we do not define parameters \n",
    "# explicitly, but use hyperparameter templates from AutoML. Those templates can be\n",
    "# used later by Hyperopt step to find best model parameters automatically\n",
    "model_list = [\n",
    "      (RandomForestClassifier, random_forest_hp_space()),\n",
    "      (KNeighborsClassifier, knn_hp_space(lambda key: key)),\n",
    "      (XGBClassifier, xgboost_hp_space())\n",
    "  ]\n",
    "\n",
    "\n",
    "# Create executor, initialize it with our classification dataset \n",
    "# and set total number of epochs to 2 (the pipeline will be run two times in a row).\n",
    "# We can load any pipeline into executor using << operator like below:\n",
    "context, pipeline_data = LocalExecutor(dataset, epochs=2) << \\\n",
    "    (Pipeline() # Here we define the pipeline. Steps can be added to pipeline using >> operator\n",
    "     # First we define our ModelSpace. We wrap it with PipelineStep class \n",
    "     # and set initializer=True so that ModelSpace step will be run only at the first epoch\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     # But we are not obliged to wrap all steps with PipelineStep.\n",
    "     # This will be done automatically if we do not need to set any special parameters \n",
    "     # We use PolynomialGenerator to create polynomial combinations of the features from the dataset\n",
    "     >> PolynomialGenerator()\n",
    "     # Next we use Hyperopt to find the best combination of hyperparameters for each model\n",
    "     # We use test set validation with accuracy metric as a score function.\n",
    "     # CV could be used instead of Validate to perform cross-validation\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=20)\n",
    "     # Then we choose the best performing model we found\n",
    "     >> ChooseBest(1)\n",
    "     # And select 10 best features\n",
    "     >> FeatureSelector(20))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have reached better accuracy compared to default XGBoost plain dataset. However, first pipeline we launched had a good job of generating various features for our dataset, but it was not really created for searching the best model. Now let's create a pipeline which will search for the best model on a fixed dataset.\n",
    "\n",
    "Please note that increasing `max_evals` parameter for `Hyperopt` can lead to finding better model parameters, but we use modest values here for demonstation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'Hyperopt'\n",
      "Hyperopt - INFO - {'n_estimators': <hyperopt.pyll.base.Apply object at 0x110afd710>, 'max_features': <hyperopt.pyll.base.Apply object at 0x110b129e8>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12278>, 'min_samples_split': 2, 'min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x110b12630>, 'bootstrap': <hyperopt.pyll.base.Apply object at 0x110b126a0>, 'oob_score': False, 'n_jobs': 1, 'random_state': <hyperopt.pyll.base.Apply object at 0x110ae5f60>, 'verbose': False, 'criterion': 'gini'}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004085 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004677 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005231 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004224 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004392 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004576 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005282 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004930 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004922 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004776 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004233 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004856 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004926 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005101 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003870 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004319 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004134 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004484 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004554 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005350 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004570 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004733 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.196409\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005655 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004903 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004796 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004253 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004385 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004349 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004464 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005702 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005470 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004899 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003714 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003747 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004631 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005213 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004022 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004254 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004253 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006715 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007372 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004691 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003693 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004531 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004095 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004867 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005945 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005553 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004377 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.195286\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003906 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.195286\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'n_neighbors': <hyperopt.pyll.base.Apply object at 0x110b127b8>, 'weights': <hyperopt.pyll.base.Apply object at 0x110b129b0>, 'algorithm': 'auto', 'leaf_size': 30, 'metric': 'euclidean', 'p': 2, 'metric_params': None, 'n_jobs': 1}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001962 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005780 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.391695\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002883 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.305275\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003809 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002699 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002689 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002789 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002775 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002217 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002209 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002209 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003297 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002305 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002389 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002245 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002239 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002438 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003408 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.205387\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002822 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003121 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002424 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002224 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002140 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002584 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002756 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001987 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.001784 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002093 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003823 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002134 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002509 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002629 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002528 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002819 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002263 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002270 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002897 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003463 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002175 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003521 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002997 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002758 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003361 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002474 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003699 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002404 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002361 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002206 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.002219 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.202020\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.003480 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.202020\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      "Hyperopt - INFO - {'max_depth': <hyperopt.pyll.base.Apply object at 0x110b12cf8>, 'learning_rate': <hyperopt.pyll.base.Apply object at 0x110b3b128>, 'n_estimators': <hyperopt.pyll.base.Apply object at 0x110b3b320>, 'gamma': <hyperopt.pyll.base.Apply object at 0x110b3b4a8>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x110b3b668>, 'max_delta_step': 0, 'subsample': <hyperopt.pyll.base.Apply object at 0x110b3b7b8>, 'colsample_bytree': <hyperopt.pyll.base.Apply object at 0x110b3b908>, 'colsample_bylevel': <hyperopt.pyll.base.Apply object at 0x110b3ba58>, 'reg_alpha': <hyperopt.pyll.base.Apply object at 0x110b3bbe0>, 'reg_lambda': <hyperopt.pyll.base.Apply object at 0x110b3bd68>, 'scale_pos_weight': 1, 'base_score': 0.5, 'seed': <hyperopt.pyll.base.Apply object at 0x110b3be10>}\n",
      "Hyperopt - INFO - Running hyperparameter optimization for <class 'xgboost.sklearn.XGBClassifier'>\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004497 seconds\n",
      "hyperopt.tpe - INFO - TPE using 0 trials\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004549 seconds\n",
      "hyperopt.tpe - INFO - TPE using 1/1 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005759 seconds\n",
      "hyperopt.tpe - INFO - TPE using 2/2 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004673 seconds\n",
      "hyperopt.tpe - INFO - TPE using 3/3 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004891 seconds\n",
      "hyperopt.tpe - INFO - TPE using 4/4 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005388 seconds\n",
      "hyperopt.tpe - INFO - TPE using 5/5 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005311 seconds\n",
      "hyperopt.tpe - INFO - TPE using 6/6 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004989 seconds\n",
      "hyperopt.tpe - INFO - TPE using 7/7 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005397 seconds\n",
      "hyperopt.tpe - INFO - TPE using 8/8 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005384 seconds\n",
      "hyperopt.tpe - INFO - TPE using 9/9 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005488 seconds\n",
      "hyperopt.tpe - INFO - TPE using 10/10 trials with best loss 0.209877\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004770 seconds\n",
      "hyperopt.tpe - INFO - TPE using 11/11 trials with best loss 0.208754\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004906 seconds\n",
      "hyperopt.tpe - INFO - TPE using 12/12 trials with best loss 0.206510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyperopt.tpe - INFO - tpe_transform took 0.005166 seconds\n",
      "hyperopt.tpe - INFO - TPE using 13/13 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005136 seconds\n",
      "hyperopt.tpe - INFO - TPE using 14/14 trials with best loss 0.206510\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004515 seconds\n",
      "hyperopt.tpe - INFO - TPE using 15/15 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004597 seconds\n",
      "hyperopt.tpe - INFO - TPE using 16/16 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006371 seconds\n",
      "hyperopt.tpe - INFO - TPE using 17/17 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006206 seconds\n",
      "hyperopt.tpe - INFO - TPE using 18/18 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005871 seconds\n",
      "hyperopt.tpe - INFO - TPE using 19/19 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005204 seconds\n",
      "hyperopt.tpe - INFO - TPE using 20/20 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005182 seconds\n",
      "hyperopt.tpe - INFO - TPE using 21/21 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006584 seconds\n",
      "hyperopt.tpe - INFO - TPE using 22/22 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005502 seconds\n",
      "hyperopt.tpe - INFO - TPE using 23/23 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004828 seconds\n",
      "hyperopt.tpe - INFO - TPE using 24/24 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004749 seconds\n",
      "hyperopt.tpe - INFO - TPE using 25/25 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004696 seconds\n",
      "hyperopt.tpe - INFO - TPE using 26/26 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004766 seconds\n",
      "hyperopt.tpe - INFO - TPE using 27/27 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007035 seconds\n",
      "hyperopt.tpe - INFO - TPE using 28/28 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005002 seconds\n",
      "hyperopt.tpe - INFO - TPE using 29/29 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005482 seconds\n",
      "hyperopt.tpe - INFO - TPE using 30/30 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004729 seconds\n",
      "hyperopt.tpe - INFO - TPE using 31/31 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.008283 seconds\n",
      "hyperopt.tpe - INFO - TPE using 32/32 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004530 seconds\n",
      "hyperopt.tpe - INFO - TPE using 33/33 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005204 seconds\n",
      "hyperopt.tpe - INFO - TPE using 34/34 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004767 seconds\n",
      "hyperopt.tpe - INFO - TPE using 35/35 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.006708 seconds\n",
      "hyperopt.tpe - INFO - TPE using 36/36 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004693 seconds\n",
      "hyperopt.tpe - INFO - TPE using 37/37 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004760 seconds\n",
      "hyperopt.tpe - INFO - TPE using 38/38 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005397 seconds\n",
      "hyperopt.tpe - INFO - TPE using 39/39 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004699 seconds\n",
      "hyperopt.tpe - INFO - TPE using 40/40 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005162 seconds\n",
      "hyperopt.tpe - INFO - TPE using 41/41 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004762 seconds\n",
      "hyperopt.tpe - INFO - TPE using 42/42 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005360 seconds\n",
      "hyperopt.tpe - INFO - TPE using 43/43 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005784 seconds\n",
      "hyperopt.tpe - INFO - TPE using 44/44 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005462 seconds\n",
      "hyperopt.tpe - INFO - TPE using 45/45 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.007726 seconds\n",
      "hyperopt.tpe - INFO - TPE using 46/46 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004672 seconds\n",
      "hyperopt.tpe - INFO - TPE using 47/47 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.005405 seconds\n",
      "hyperopt.tpe - INFO - TPE using 48/48 trials with best loss 0.204265\n",
      "hyperopt.tpe - INFO - tpe_transform took 0.004617 seconds\n",
      "hyperopt.tpe - INFO - TPE using 49/49 trials with best loss 0.204265\n",
      "Hyperopt - INFO - Reversing best score bask to original form as reverse_score=True\n",
      " 67%|██████▋   | 2/3 [03:20<01:40, 100.11s/it]LocalExecutor - INFO - Running step 'ChooseBest'\n",
      "ChooseBest - INFO - Final model scores:\n",
      "ChooseBest - INFO - RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=77, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) - 0.8047138047138048\n",
      "ChooseBest - INFO - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform') - 0.797979797979798\n",
      "ChooseBest - INFO - XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "       colsample_bylevel=0.9439801428101403,\n",
      "       colsample_bytree=0.5542595913412373, gamma=4.983853559643964,\n",
      "       learning_rate=0.00983614007205386, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.07602348522584429, reg_lambda=1.245497994618211,\n",
      "       scale_pos_weight=1, seed=3, silent=True,\n",
      "       subsample=0.6872993176982336) - 0.7957351290684623\n",
      "100%|██████████| 3/3 [03:20<00:00, 66.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=77, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=False,\n",
      "            warm_start=False) 0.8047138047138048\n",
      "(891, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context, pipeline_data = LocalExecutor(pipeline_data.dataset, epochs=1) << \\\n",
    "    (Pipeline()\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=50)\n",
    "     >> ChooseBest(1))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a nice improvement. That's enough to get into the top 1% in the Kaggle Titatic demo competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting feature selection\n",
    "\n",
    "AutoML also allows you to select feature that perform well for the most models in the model space. Features that have geather importance in multiple models will have geather weight.\n",
    "\n",
    "\n",
    "First $N$ features are selected from the following well-ordered set:\n",
    "\n",
    "$\\mathbb{F} = softmax(\\mathbb{I}) \\circ \\mathbb{S}$,\n",
    "\n",
    "where \n",
    "* $\\mathbb{I} \\in \\mathbb{R}$ represents a model-specific feature score set\n",
    "* $\\mathbb{S} \\in \\mathbb{R}$ is a set of model scores according to some scoring function $s(x, m): \\mathbb{M} \\rightarrow \\mathbb{R} $ ($x$ is a dataset, $m$ is a model, $\\mathbb{M}$ is a model space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LocalExecutor - INFO - Starting AutoML Epoch #1\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]LocalExecutor - INFO - Running step 'model space'\n",
      "LocalExecutor - INFO - Running step 'FormulaFeatureGenerator'\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-04fdb42d1054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m      \u001b[0;34m>>\u001b[0m \u001b[0mFormulaFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m      \u001b[0;34m>>\u001b[0m \u001b[0mHyperopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m      >> VotingFeatureSelector(feature_to_select=5, reverse_score=True))\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipeline_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/work/OnePanel/auto-ml/automl/pipeline.py\u001b[0m in \u001b[0;36m__lshift__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__lshift__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/work/OnePanel/auto-ml/automl/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, pipeline, input_data, epochs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                                 in self._context.model_space]\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mpipeline_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/work/OnePanel/auto-ml/automl/pipeline.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pipe_input, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_model_space_functor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/work/OnePanel/auto-ml/automl/feature/generators.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pipeline_data, pipeline_context, limit)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mnew_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mused_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mpipeline_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/work/OnePanel/auto-ml/automl/feature/generators.py\u001b[0m in \u001b[0;36m_divide\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mfirst_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_two_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"({dataset.meta[first_index]['history']}/{dataset.meta[second_index]['history']})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dataset.meta[first_index]['name']}_/_{dataset.meta[second_index]['name']}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from automl.hyperparam.hyperopt import Hyperopt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from automl.feature.selector import VotingFeatureSelector\n",
    "from automl.feature.generators import FormulaFeatureGenerator\n",
    "\n",
    "model_list = [\n",
    "      (RandomForestClassifier, random_forest_hp_space()),\n",
    "      (LogisticRegression, {}),\n",
    "      (XGBClassifier, xgboost_hp_space())\n",
    "  ]\n",
    "\n",
    "context, pipeline_data = LocalExecutor(dataset, epochs=2) << \\\n",
    "    (Pipeline()\n",
    "     >> PipelineStep('model space', ModelSpace(model_list), initializer=True)\n",
    "     >> FormulaFeatureGenerator()\n",
    "     >> Hyperopt(CV(scoring=make_scorer(accuracy_score)), max_evals=20)\n",
    "     >> VotingFeatureSelector(feature_to_select=5, reverse_score=True))\n",
    "\n",
    "for result in pipeline_data.return_val:\n",
    "    print(result.model, result.score)\n",
    "print(pipeline_data.dataset.data.shape)\n",
    "\n",
    "print(f\"Selected features:\")\n",
    "for col in pipeline_data.dataset.columns:\n",
    "    print(f\"{col}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 features were selected from an initial set on 9.\n",
    "\n",
    "# Reproducable preprocessing\n",
    "After you're done with AutoML model search it may be useful to reproduce resulting feature generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.feature.generators import Preprocessing\n",
    "\n",
    "original_dataset = preprocess_data(data)\n",
    "\n",
    "# Let's recreate all features useful features found in AutoML Pipeline\n",
    "preprocessing = Preprocessing()\n",
    "final_data = preprocessing.reproduce(pipeline_data.dataset, original_dataset)\n",
    "final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
