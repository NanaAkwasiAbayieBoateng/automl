import logging
from functools import partial
import random
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from automl.pipeline import PipelineData




class SklearnFeatureGenerator:
    def __init__(self, transformer_class, *args, **kwargs):
        """
        Wrapper for Scikit-Learn Transformers

        Parameters
        ----------
        kwargs:
            keyword arguments are passed to sklearn PolynomialFeatures
        """
        self._log = logging.getLogger(self.__class__.__name__)
        self._transformer = transformer_class(*args, **kwargs)

    def __call__(self, pipeline_data, pipeline_context):
        """
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The data.

        pipeline_context: automl.pipeline.PipelineContext
            global context of a pipeline

        Returns
        -------
        X_new : numpy array of shape [n_samples, n_features_new]
            Transformed array.
        """

        pipeline_data.dataset.data = self._transformer.fit_transform(pipeline_data.dataset.data)
        return pipeline_data


class FormulaFeatureGenerator:
    def __init__(self, func_list=['+', '-', '*', '/']):
        """
        Initialize Formula Feature Generator

        Paramets
        --------
        func_list : list of symbols of functions
            In current version func_list may contain only '+', '-', '*', '/'.

        Attributes
        ----------
        _func_map : dict of generating functions
            The function for generation new features

        used_func : set of of symbols of functions
        """
        self._log = logging.getLogger(self.__class__.__name__)
        self.used_func = set(func_list)
        self._func_map = {
            '+': self._sum,
            '-': self._substract,
            '/': self._divide,
            '*': self._multiply,
        }

    def _sum(self, dataset):
        """
        Add one new feature generated by sum of two random features

        Parametrs
        ---------
        X : np.ndarray, shape [n_samples, n_features]
            The data to transform, row by row.

        Returns
        -------
        XF : np.ndarray shape [n_samples, n_features+1]
            The matrix of features with one new feature
        """
        X = dataset.data
        first_index, second_index = self._choose_two_index(X)
        x, y = X[:, first_index].reshape(X.shape[0], 1), X[:, second_index].reshape(X.shape[0], 1)
        history = f"({dataset.meta[first_index]['history']}+{dataset.meta[second_index]['history']})"
        return x + y, history
    
    def _substract(self, dataset):
        """
        Add one new feature generated by substraction of two random features

        Parametrs
        ---------
        X : np.ndarray, shape [n_samples, n_features]
            The data to transform, row by row.

        Returns
        -------
        XF : np.ndarray shape [n_samples, n_features+1]
            The matrix of features with one new feature
        """
        X = dataset.data
        first_index, second_index = self._choose_two_index(X)
        x, y = X[:, first_index].reshape(X.shape[0], 1), X[:, second_index].reshape(X.shape[0], 1)
        history = f"({dataset.meta[first_index]['history']}-{dataset.meta[second_index]['history']})"
        return x - y, history

    def _divide(self, dataset):
        """
        Add one new feature generated by division of two random features

        Parametrs
        ---------
        X : np.ndarray, shape [n_samples, n_features]
            The data to transform, row by row.

        Returns
        -------
        XF : np.ndarray shape [n_samples, n_features+1]
            The matrix of features with one new feature
        """
        X = dataset.data
        first_index, second_index = self._choose_two_index(X)
        x, y = X[:, first_index].reshape(X.shape[0], 1), X[:, second_index].reshape(X.shape[0], 1)
        history = f"({dataset.meta[first_index]['history']}/{dataset.meta[second_index]['history']})"
        return x / y, history

    def _multiply(self, dataset):
        """
        Add one new feature generated by multiplication of two random features

        Parametrs
        ---------
        X : np.ndarray, shape [n_samples, n_features]
            The data to transform, row by row.

        Returns
        -------
        XF : np.ndarray shape [n_samples, n_features+1]
            The matrix of features with one new feature
        """
        X = dataset.data
        first_index, second_index = self._choose_two_index(X)
        x, y = X[:, first_index].reshape(X.shape[0], 1), X[:, second_index].reshape(X.shape[0], 1)
        history = f"({dataset.meta[first_index]['history']}*{dataset.meta[second_index]['history']})"
        return x * y, history

    def _choose_two_index(self, X):
        """
        Choose two features from input data

        Parametrs
        ---------
        X : np.ndarray, shape [n_samples, n_features]
            The data

        Returns
        -------
        x, y : Two vectors of selected features
        """
        return random.randint(0, X.shape[1]-1),\
               random.randint(0, X.shape[1]-1)

    def __call__(self, pipeline_data, pipeline_context, limit=10):
        """
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The data.

        limit : int
            Amount of new features

        pipeline_context: automl.pipeline.PipelineContext
            global context of a pipeline

        Returns
        -------
        XF : numpy array of shape [n_samples, n_features+limit]
            Transformed array.
        """
        orig_feature_num = pipeline_data.dataset.data.shape[1]
        if not isinstance(pipeline_data.dataset, np.ndarray):
            pipeline_data.dataset.data = np.array(pipeline_data.dataset.data)

        for _ in range(0, limit):
            new_feature, history = self._func_map[random.sample(self.used_func, 1)[0]](pipeline_data.dataset)
            if np.isfinite(new_feature).all():
                pipeline_data.dataset.data = np.append(pipeline_data.dataset.data, new_feature, axis=1)

                pipeline_data.dataset.meta.append({
                    "name" : "",
                    "history" : history
                })

        self._log.info((f"Generated new features. Old feature number - "
                        f"{orig_feature_num}, new feature number - "
                        f"{pipeline_data.dataset.data.shape[1]}"))
        return pipeline_data 

class RecoveringFeatureGenerator:
    def __init__(self):
        self._log = logging.getLogger(self.__class__.__name__)

    def __call__(self, pipeline_data, pipeline_context):
        dataset = pipeline_data.dataset
        data = dataset.data
        for feature in dataset.meta:
            if feature["name"] != "base_feature": 
                explicit_locals = locals()
                exec(f"new_feature = {feature['history']}", globals(), explicit_locals)
                new_feature = explicit_locals["new_feature"]
                np.append(data, new_feature)
        return pipeline_data

PolynomialGenerator = partial(SklearnFeatureGenerator, PolynomialFeatures)
